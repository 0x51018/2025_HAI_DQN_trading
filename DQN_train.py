# íŒŒì¼ëª…: dqn_trading_agent.py

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random

# â”€â”€â”€ 0. ë°ì´í„° ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = pd.read_csv("prices/AAPL.csv", index_col=0)
open_prices  = df["open"].values.tolist()
close_prices = df["close"].values.tolist()

# â”€â”€â”€ 1. ë””ë°”ì´ìŠ¤ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")

# â”€â”€â”€ 2. ë„¤íŠ¸ì›Œí¬ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class DQNetwork(nn.Module):
    def __init__(self, state_size, action_size, hidden_layers=[256]):
        """
        state_size: ì…ë ¥ íŠ¹ì„± í¬ê¸°
        action_size: ê°€ëŠ¥í•œ í–‰ë™ ìˆ˜ (ë³´í†µ 3: 0=Hold, 1=Buy, 2=Sell)
        hidden_layers: ì€ë‹‰ì¸µ ë…¸ë“œ ìˆ˜ ë¦¬ìŠ¤íŠ¸
        """
        super().__init__()
        layers = []
        in_dim = state_size
        # ì€ë‹‰ì¸µ êµ¬ì„±
        for h in hidden_layers:
            layers.append(nn.Linear(in_dim, h))
            layers.append(nn.ReLU())
            in_dim = h
        # ì¶œë ¥ì¸µ
        layers.append(nn.Linear(in_dim, action_size))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)

# â”€â”€â”€ 3. ì—ì´ì „íŠ¸ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Agent:
    def __init__(self,
                 state_size, window_size, trend, skip, batch_size,
                 open_list, close_list,
                 reward_fn=None,
                 network_class=DQNetwork, network_kwargs=None):
        """
        reward_fn: (agent, t, action) -> float í˜•íƒœì˜ í•¨ìˆ˜. Noneì´ë©´ ê¸°ë³¸ ë³´ìƒ í•¨ìˆ˜ ì‚¬ìš©
        network_class: DQN ë„¤íŠ¸ì›Œí¬ í´ë˜ìŠ¤ë¡œ, DQNetwork ì™¸ ë‹¤ë¥¸ í´ë˜ìŠ¤ë„ ì£¼ì… ê°€ëŠ¥
        network_kwargs: ë„¤íŠ¸ì›Œí¬ ìƒì„±ìì— ë„˜ê¸¸ ì¶”ê°€ í‚¤ì›Œë“œ ì¸ì dict
        """
        # ì›ë˜ ì¸ìë“¤
        self.state_size  = state_size
        self.window_size = window_size
        self.half_window = window_size // 2
        self.trend       = trend
        self.open        = open_list
        self.close       = close_list
        self.skip        = skip
        self.action_size = 3
        self.batch_size  = batch_size
        self.memory      = deque(maxlen=1000)
        self.inventory   = []

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.gamma         = 0.95
        self.epsilon       = 0.5
        self.epsilon_min   = 0.01
        self.epsilon_decay = 0.999

        # ë³´ìƒ í•¨ìˆ˜ ì£¼ì…
        if reward_fn is not None:
            self.reward_fn = reward_fn
        else:
            # ê¸°ë³¸ ë³´ìƒ í•¨ìˆ˜: ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ
            self.reward_fn = self._default_reward

        # ë””ë°”ì´ìŠ¤ ë° ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.device = torch.device(DEVICE)
        net_kwargs = network_kwargs if network_kwargs else {}
        self.model     = network_class(state_size, self.action_size, **net_kwargs).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-5)
        self.criterion = nn.MSELoss()

    def _default_reward(self, agent, t, action):
        """ì›ë³¸ ì½”ë“œì˜ ë³´ìƒ ê³„ì‚° ë¡œì§ì„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬"""
        reward = 0.0
        # ë§¤ìˆ˜
        if action == 1 and t < len(self.trend) - self.half_window:
            # ë§¤ìˆ˜ ì‹œì  ê¸°ë¡
            self.inventory.append(self.trend[t])
            # ê¸°ì¤€ê°€: ì§ì „ ì‹œê°€/ì¢…ê°€/í˜„ì¬ê°€ ì¤‘ ìµœëŒ€
            ref = (max(self.open[t-1], self.close[t-1], self.trend[t])
                   if t > 0 else self.trend[t])
            reward = (ref - self.trend[t]) / self.trend[t]
        # ë§¤ë„
        elif action == 2 and self.inventory:
            bought = self.inventory.pop(0)
            reward = (self.trend[t] - bought) / self.trend[t]
        return reward

    def act(self, state):
        """Îµ-ê·¸ë¦¬ë”” ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ"""
        if random.random() <= self.epsilon:
            return random.randrange(self.action_size)
        state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)
        with torch.no_grad():
            q_values = self.model(state_tensor)
        return torch.argmax(q_values).item()

    def get_state(self, t):
        ws = self.window_size + 1
        d = t - ws + 1
        if d >= 0:
            block = self.trend[d:t + 1]
        else:
            pad_len = -d
            block = [self.trend[0]] * pad_len + self.trend[0:t + 1]
        # ğŸ”’ ì•ˆì „ ì¥ì¹˜ ì¶”ê°€
        if len(block) != ws:
            block = block[:ws]  # ì˜ëª»ë˜ë©´ ìë¥´ê¸°

        diffs = [block[i + 1] - block[i] for i in range(len(block) - 1)]
        return np.array(diffs, dtype=np.float32)
    

    def replay(self):
        """ê²½í—˜ ì¬í”Œë ˆì´ ë° ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        if len(self.memory) < self.batch_size:
            return
        mini = random.sample(self.memory, self.batch_size)
        s, a, r, ns, done = zip(*mini)

        states      = torch.tensor(s, dtype=torch.float32).to(self.device)
        actions     = torch.tensor(a, dtype=torch.long).to(self.device)
        rewards     = torch.tensor(r, dtype=torch.float32).to(self.device)
        next_states = torch.tensor(ns, dtype=torch.float32).to(self.device)
        dones       = torch.tensor(done, dtype=torch.float32).to(self.device)

        curr_q = self.model(states).gather(1, actions.unsqueeze(1)).squeeze()
        next_q = self.model(next_states).max(1)[0].detach()
        target = rewards + (1 - dones) * self.gamma * next_q

        loss = self.criterion(curr_q, target)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def train(self, episodes):
        """ì—í”¼ì†Œë“œ ë‹¨ìœ„ í•™ìŠµ"""
        for ep in range(episodes):
            state = self.get_state(0)
            total_profit = 0.0

            for t in range(0, len(self.trend)-1, self.skip):
                action = self.act(state)
                nxt    = self.get_state(t+1)
                # ê¸°ë³¸ ë³´ìƒ ê³„ì‚° (inventory ì¡°ì‘ì€ _default_reward ë‚´ë¶€ì—ì„œ ì²˜ë¦¬)
                default_reward = self._default_reward(self, t, action)
                # ì£¼ì…ëœ ë³´ìƒ í•¨ìˆ˜ í˜¸ì¶œ
                reward = self.reward_fn(self, t, action)
                # total_profit ì§‘ê³„ (ë°œí‘œìš© ë¡œê·¸ ê¸°ì¤€)
                if action == 2 and default_reward != 0:
                    total_profit += default_reward

                self.memory.append((state, action, reward, nxt, (t == len(self.trend)-2)))
                state = nxt
                self.replay()

            print(f"Episode {ep+1}/{episodes}, Total Profit: {total_profit:.4f}")

    def buy(self, initial_money):
        """í›ˆë ¨ëœ ì •ì±…ìœ¼ë¡œ ë§¤ë§¤ ì‹œë®¬ë ˆì´ì…˜"""
        state   = self.get_state(0)
        balance = initial_money
        buys, sells = [], []

        for t in range(0, len(self.trend)-1, self.skip):
            action = self.act(state)
            state  = self.get_state(t+1)
            price  = self.trend[t]

            if action == 1 and balance >= price:
                self.inventory.append(price)
                balance -= price
                buys.append(t)
            elif action == 2 and self.inventory:
                bought = self.inventory.pop(0)
                balance += price
                sells.append(t)

        gain    = balance - initial_money
        invest  = gain / initial_money * 100
        return buys, sells, gain, invest

# â”€â”€â”€ 4. í•™ìŠµ ë° íŠ¸ë ˆì´ë“œ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
initial_money = 10000
window_size   = 30
skip          = 1
batch_size    = 32

agent = Agent(
    state_size=window_size,
    window_size=window_size,
    trend=close_prices,
    skip=skip,
    batch_size=batch_size,
    open_list=open_prices,
    close_list=close_prices,
    # reward_fn=None,                  # ê¸°ë³¸ ë³´ìƒ ì‚¬ìš©
    # network_class=DQNetwork,         # ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
    # network_kwargs={"hidden_layers":[256]}  # ê¸°ë³¸ ì€ë‹‰ 256
)

agent.train(episodes=20)
buys, sells, total_gain, invest_pct = agent.buy(initial_money)

# â”€â”€â”€ 5. ê²°ê³¼ ì‹œê°í™” ë° íŒŒì¼ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import matplotlib.pyplot as plt
plt.figure(figsize=(15,5))
plt.plot(close_prices, color='black', lw=2)
plt.plot(close_prices, '^', markersize=10, color='blue',
         label='Buy Signal',  markevery=buys)
plt.plot(close_prices, 'v', markersize=10, color='red',
         label='Sell Signal', markevery=sells)
plt.title(f"Total Gain: {total_gain:.4f}, ROI: {invest_pct:.2f}%")
plt.legend()
plt.savefig("trade_signals.png", dpi=150, bbox_inches="tight")
plt.close()
print("Saved Png")
# â”€â”€â”€ 6. ê²°ê³¼ ìš”ì•½ íŒŒì¼ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€